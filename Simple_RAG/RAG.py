# RAG.py
import streamlit as st
from langchain_community.chat_models import ChatOpenAI
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain_community.vectorstores import Milvus
from langchain.memory import ConversationBufferMemory
from bs4 import BeautifulSoup
import re
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# ------------------------------
# é¡µé¢å’Œä¼šè¯é…ç½®
# ------------------------------
st.set_page_config(page_title="åŒ»å­¦é—®ç­”åŠ©æ‰‹", layout="wide")  
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'step' not in st.session_state:
    st.session_state.step = 'ask'
if 'stored_docs' not in st.session_state:
    st.session_state.stored_docs = []
if 'prev_query' not in st.session_state:
    st.session_state.prev_query = ''

# ------------------------------
# åŸºç¡€ç»„ä»¶åˆå§‹åŒ–
# ------------------------------
device = "cuda" if torch.cuda.is_available() else "cpu"
llm = ChatOpenAI(
    model_name="deepseek-chat",
    temperature=0,
    openai_api_key="sk-7386338a6e5b41c7ba359c998f4a5fbc",
    openai_api_base="https://api.deepseek.com/v1"
)
embedding_model = HuggingFaceBgeEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={"device": device},
    encode_kwargs={"normalize_embeddings": True},
    query_instruction=""
)
vector_store = Milvus(
    collection_name="medical_docs",
    embedding_function=embedding_model,
    connection_args={"host": "localhost", "port": "19530"}
)
retriever = vector_store.as_retriever(search_kwargs={"k": 5})

# é‡æ’æ¨¡å‹
reranker_tokenizer = AutoTokenizer.from_pretrained("BAAI/bge-reranker-base")
reranker_model = AutoModelForSequenceClassification.from_pretrained("BAAI/bge-reranker-base")
reranker_model.to(device)
def rerank_documents(query, docs):
    reranked = []
    for doc in docs:
        inputs = reranker_tokenizer(query, doc.page_content, return_tensors="pt", truncation=True).to(device)
        with torch.no_grad(): score = reranker_model(**inputs).logits.squeeze().item()
        reranked.append((score, doc))
    reranked.sort(key=lambda x: x[0], reverse=True)
    return [d for _, d in reranked[:10]]

# æ–‡æœ¬æ¸…æ´—ä¸åˆ‡åˆ†ï¼ˆè§å‰çœç•¥å®ç°ï¼Œä¿æŒåŒåŸï¼‰
def clean_html(html_content: str) -> str:
    soup = BeautifulSoup(html_content, "html.parser")
    for elem in soup(["script", "style"]): elem.extract()
    text = soup.get_text(separator="\n")
    return re.sub(r'\n+', '\n', text).strip()
def clean_markdown(md_content: str) -> str:
    text = re.sub(r'```.*?```', '', md_content, flags=re.DOTALL)
    text = re.sub(r'`([^`]+)`', r'\1', text)
    text = re.sub(r'!\[.*?\]\(.*?\)', '', text)
    text = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', text)
    text = re.sub(r'^#{1,6}\s*', '', text, flags=re.MULTILINE)
    return re.sub(r'\n+', '\n', text).strip()

def load_and_split_documents(folder_path):
    docs=[]; spl=RecursiveCharacterTextSplitter(separators=["\n\n","ã€‚","ï¼"],chunk_size=500,chunk_overlap=50,length_function=len)
    for f in os.listdir(folder_path):
        p=os.path.join(folder_path,f)
        if f.endswith((".html",".md")):
            raw=open(p,encoding='utf-8').read()
            text = clean_html(raw) if f.endswith(".html") else clean_markdown(raw)
            for i,ch in enumerate(spl.split_text(text)): docs.append((ch,{"source":f,"chunk":i}))
    return docs

# ------------------------------
# UI æ¸²æŸ“
# ------------------------------
st.title("ğŸ©º åŒ»å­¦çŸ¥è¯†é—®ç­”åŠ©æ‰‹")

# ç”¨æˆ·æé—®é˜¶æ®µ
if st.session_state.step == 'ask':
    # å¦‚æœæœ‰å‰ç½®é—®é¢˜ï¼Œæ˜¾ç¤ºä¸ºä¸Šä¸‹æ–‡
    if st.session_state.prev_query:
        st.info(f"ğŸ“Œ ä¸Šä¸€è¯é¢˜ï¼š{st.session_state.prev_query}")
    query = st.text_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š", key="input_query")
    if query:
        # è‹¥å­˜åœ¨ prev_queryï¼Œåˆ™è¿­ä»£æ‹¼æ¥
        full_query = (st.session_state.prev_query + ' ' + query).strip() if st.session_state.prev_query else query
        st.session_state.chat_history.append(("ç”¨æˆ·", query))
        # æ›´æ–° prev_query ä¸º full_query
        st.session_state.prev_query = full_query
        st.write("ğŸ” æ­£åœ¨æ£€ç´¢å¹¶é‡æ’æ–‡æ¡£...")
        docs = retriever.get_relevant_documents(full_query)
        reranked = rerank_documents(full_query, docs)
        st.session_state.stored_docs = reranked
        st.session_state.step = 'review'
        st.rerun()

# æ–‡æ¡£å®¡é˜…é˜¶æ®µ
elif st.session_state.step == 'review':
    st.write("ğŸ“„ ä»¥ä¸‹æ˜¯æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µï¼Œè¯·é€‰æ‹©è¿›ä¸€æ­¥ä½¿ç”¨çš„å†…å®¹ï¼š")
    selections = []
    with st.form("review_form"):
        for idx, doc in enumerate(st.session_state.stored_docs):
            sel = st.checkbox(f"æ–‡æ¡£ {idx+1} æ¥æº:{doc.metadata['source']}â€¦", key=f"sel{idx}")
            st.write(doc.page_content[:200] + "â€¦")
            if sel:
                selections.append(doc.page_content)
        submitted = st.form_submit_button("âœ… ä½¿ç”¨é€‰ä¸­å†…å®¹ç”Ÿæˆç­”æ¡ˆ")
    if submitted:
        st.session_state.selected_context = (
            selections if selections else [d.page_content for d in st.session_state.stored_docs]
        )
        st.session_state.step = 'answer'
        st.rerun()
    if st.button("ğŸ”„ ä¿®æ”¹æŸ¥è¯¢å†…å®¹"):
        st.session_state.step = 'ask'
        st.rerun()

# å›ç­”ç”Ÿæˆé˜¶æ®µ
elif st.session_state.step == 'answer':
    context = "\n\n".join(st.session_state.selected_context)
    user_msg = st.session_state.chat_history[-1][1]
    prompt = f"ä½ æ˜¯ä¸€ä½åŒ»å­¦ä¸“å®¶ï¼Œè¯·æ ¹æ®ä»¥ä¸‹èµ„æ–™å›ç­”ï¼š\n{context}\né—®é¢˜ï¼š{user_msg}\nè¯¦ç»†å›ç­”ï¼š"
    st.write("âœï¸ æ­£åœ¨ç”Ÿæˆå›ç­”...")
    ans = llm.predict(prompt)
    st.session_state.chat_history.append(("åŠ©æ‰‹", ans))
    st.write(f"**å›ç­”**ï¼š{ans}")
    # â€œè¿½é—®â€è·³å›askï¼Œä¿ç•™prev_queryç”¨äºè¿­ä»£
    if st.button("ğŸ’¡ è¿½é—®"):
        st.session_state.step = 'ask'
        st.rerun()
    # æ–°é—®é¢˜æ—¶é‡ç½®ä¸Šä¸‹æ–‡
    if st.button("ğŸ  æ–°é—®é¢˜"): 
        st.session_state.step = 'ask'
        st.session_state.prev_query = ''
        st.session_state.stored_docs = []
        st.rerun()

# æ˜¾ç¤ºå¤šè½®å†å²ï¼ˆå¯æŠ˜å ï¼‰
st.markdown("---")
with st.expander("ğŸ“œ æŸ¥çœ‹å¯¹è¯å†å²", expanded=False):
    for role, msg in st.session_state.chat_history:
        st.chat_message(role).markdown(msg)
